{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68d76c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter le dossier parent au path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Ensuite tu peux importer ton module\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fda7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & paramètres\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import utils  # doit fournir utils.load_data(pattern)\n",
    "\n",
    "# Réglages\n",
    "INPUT_GLOB = \"../../cleaned_data/*.csv\"\n",
    "OUT_DIR_COORDS = \"./reduced_data_pca\"\n",
    "OUT_METRICS_CSV = \"./reduced_data_pca/pca_metrics.csv\"\n",
    "VAR_THRESHOLD = 0.90\n",
    "N_NEIGHBORS_TRUST = 10\n",
    "RANDOM_STATE = 0\n",
    "K_NEIGHBORS_EVAL = 10  # k pour trustworthiness, continuity, mrre (sera clampé au n-1)\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR_COORDS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2831d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Utilitaires\n",
    "def ensure_dict_datasets(obj, default_name=\"merged\"):\n",
    "    \"\"\"Accepte un DataFrame unique OU un dict[str, DataFrame].\"\"\"\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return {default_name: obj}\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "    raise TypeError(\"utils.load_data doit retourner un DataFrame ou un dict[str, DataFrame].\")\n",
    "\n",
    "def pick_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Retourne les colonnes méta disponibles : player_name, equipe, positions.\"\"\"\n",
    "    tmp = df.copy()\n",
    "    if \"positions\" not in tmp.columns and \"position\" in tmp.columns:\n",
    "        tmp = tmp.rename(columns={\"position\": \"positions\"})\n",
    "    meta_cols = [c for c in [\"player_name\", \"equipe\", \"positions\"] if c in tmp.columns]\n",
    "    return tmp[meta_cols] if meta_cols else pd.DataFrame(index=tmp.index)\n",
    "\n",
    "def compute_distance_correlation(X_high: np.ndarray, X_low: np.ndarray) -> float:\n",
    "    \"\"\"Corrélation de Pearson entre matrices de distances (triangle supérieur).\"\"\"\n",
    "    D_high = pairwise_distances(X_high, metric=\"euclidean\")\n",
    "    D_low  = pairwise_distances(X_low,  metric=\"euclidean\")\n",
    "    iu = np.triu_indices_from(D_high, k=1)\n",
    "    v1, v2 = D_high[iu], D_low[iu]\n",
    "    if v1.std() == 0 or v2.std() == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(v1, v2)[0, 1])\n",
    "\n",
    "def clamp_neighbors(n_samples: int, n_neighbors: int) -> int:\n",
    "    return max(1, min(n_neighbors, max(1, n_samples - 1)))\n",
    "\n",
    "def continuity(X: np.ndarray, X_embedded: np.ndarray, k: int) -> float:\n",
    "    \"\"\"Continuity (Kaski & Venna): 1 = parfait, 0 = mauvais.\"\"\"\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    n = X.shape[0]\n",
    "    k = clamp_neighbors(n, k)\n",
    "\n",
    "    orig_nn = np.argsort(pairwise_distances(X), axis=1)[:, 1:k+1]\n",
    "    emb_nn  = np.argsort(pairwise_distances(X_embedded), axis=1)[:, 1:k+1]\n",
    "\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        orig = list(orig_nn[i])\n",
    "        emb_set = set(emb_nn[i])\n",
    "        missing = [m for m in orig if m not in emb_set]\n",
    "        # pénalité = rang (1..k) des voisins manquants dans l'espace original\n",
    "        total += sum(orig.index(m) + 1 for m in missing)\n",
    "\n",
    "    denom = n * k * (2 * n - 3 * k - 1)\n",
    "    if denom <= 0:\n",
    "        return np.nan\n",
    "    return 1 - (2 / denom) * total\n",
    "\n",
    "def mrre(X: np.ndarray, X_embedded: np.ndarray, k: int) -> float:\n",
    "    \"\"\"Mean Relative Rank Error (MRRE): 0 = parfait (mieux), plus grand = pire.\"\"\"\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    n = X.shape[0]\n",
    "    k = clamp_neighbors(n, k)\n",
    "\n",
    "    D_high = pairwise_distances(X)\n",
    "    D_low  = pairwise_distances(X_embedded)\n",
    "\n",
    "    # Rangs (1..n)\n",
    "    R_high = np.argsort(np.argsort(D_high, axis=1), axis=1) + 1\n",
    "    R_low  = np.argsort(np.argsort(D_low,  axis=1), axis=1) + 1\n",
    "\n",
    "    err = 0.0\n",
    "    cnt = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            denom = R_high[i, j]\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            err += abs(R_high[i, j] - R_low[i, j]) / denom\n",
    "            cnt += 1\n",
    "    return float(err / cnt) if cnt > 0 else float('nan')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13bbcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell - 3\n",
    "def run_pca_once(df: pd.DataFrame, var_threshold=0.90, random_state=0):\n",
    "    Xnum = df.select_dtypes(include=[np.number]).copy()\n",
    "    if Xnum.shape[1] < 2:\n",
    "        return None\n",
    "\n",
    "    X_imp = SimpleImputer(strategy=\"mean\").fit_transform(Xnum.values)\n",
    "    X_std = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "    pca = PCA(n_components=None, random_state=random_state)\n",
    "    scores_all = pca.fit_transform(X_std)\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    k = int(np.searchsorted(cumvar, var_threshold) + 1)\n",
    "\n",
    "    scores_k = scores_all[:, :k]\n",
    "    cols = [f\"pca{i+1}\" for i in range(k)]\n",
    "    coords_df = pd.DataFrame(scores_k, index=df.index, columns=cols)\n",
    "\n",
    "    # Métriques\n",
    "    nn_trust = clamp_neighbors(X_std.shape[0], N_NEIGHBORS_TRUST)\n",
    "    tw = trustworthiness(X_std, scores_k, n_neighbors=nn_trust)\n",
    "\n",
    "    nn_eval = clamp_neighbors(X_std.shape[0], K_NEIGHBORS_EVAL)\n",
    "    cont = continuity(X_std, scores_k, k=nn_eval)\n",
    "    mrre_score = mrre(X_std, scores_k, k=nn_eval)\n",
    "    \n",
    "    dist_corr = compute_distance_correlation(X_std, scores_k)\n",
    "\n",
    "\n",
    "    # cast sûrs (évite float(None))\n",
    "    def safe_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return float('nan')\n",
    "\n",
    "    return {\n",
    "        \"k\": k,\n",
    "        \"coords\": coords_df,\n",
    "        \"explained_var\": safe_float(pca.explained_variance_ratio_[:k].sum()),\n",
    "        \"trustworthiness\": safe_float(tw),\n",
    "        \"continuity\": safe_float(cont),\n",
    "        \"mrre\": safe_float(mrre_score),\n",
    "        \"distance_correlation\": safe_float(dist_corr),\n",
    "\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf419b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data: [Errno 2] No such file or directory: '../../cleaned_data/*.csv'\n",
      "Fallback: lecture directe des CSV via glob.\n",
      "4 jeux trouvés.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['joueurs_ligue1_2024_2025_clean_custom',\n",
       " 'joueurs_ligue1_2024_2025_clean_custom_no_GK',\n",
       " 'joueurs_ligue1_2024_2025_clean_per90',\n",
       " 'joueurs_ligue1_2024_2025_clean_raw']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 — Chargement des jeux de données (robuste)\n",
    "import glob\n",
    "\n",
    "def _to_dict_datasets(obj):\n",
    "    \"\"\"Normalise en dict[str, DataFrame] les formes possibles.\"\"\"\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return {\"merged\": obj}\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        out = {}\n",
    "        for i, item in enumerate(obj):\n",
    "            # format: (name, df)\n",
    "            if isinstance(item, (list, tuple)) and len(item) == 2 and isinstance(item[1], pd.DataFrame):\n",
    "                out[str(item[0])] = item[1]\n",
    "            # format: df seul\n",
    "            elif isinstance(item, pd.DataFrame):\n",
    "                out[f\"df_{i}\"] = item\n",
    "        if out:\n",
    "            return out\n",
    "    return None\n",
    "\n",
    "try:\n",
    "    raw = utils.load_data(INPUT_GLOB)  # peut renvoyer df / dict / liste\n",
    "except Exception as e:\n",
    "    print(f\"utils.load_data a échoué: {e}\")\n",
    "    raw = None\n",
    "\n",
    "datasets = _to_dict_datasets(raw)\n",
    "\n",
    "# Fallback: lecture directe des CSV si rien d'exploitable\n",
    "if datasets is None:\n",
    "    print(\"Fallback: lecture directe des CSV via glob.\")\n",
    "    paths = sorted(glob.glob(INPUT_GLOB))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé avec le motif: {INPUT_GLOB}\")\n",
    "    datasets = {\n",
    "        os.path.splitext(os.path.basename(p))[0]: pd.read_csv(p)\n",
    "        for p in paths\n",
    "    }\n",
    "\n",
    "print(f\"{len(datasets)} jeux trouvés.\")\n",
    "list(datasets.keys())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "866d8a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'distance_correlation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     22\u001b[39m out_df.to_csv(out_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     24\u001b[39m metrics_rows.append({\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: safe_name,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_components\u001b[39m\u001b[33m\"\u001b[39m: k,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mout_file\u001b[39m\u001b[33m\"\u001b[39m: out_path,\n\u001b[32m     32\u001b[39m })\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, var=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mexplained_var\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrust=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mtrustworthiness\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, dist_corr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistance_correlation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'distance_correlation'"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Boucle: PCA + sauvegardes + collecte métriques\n",
    "metrics_rows = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    res = run_pca_once(df, var_threshold=VAR_THRESHOLD, random_state=RANDOM_STATE)\n",
    "    if res is None:\n",
    "        print(f\"[SKIP] {name}: < 2 colonnes numériques.\")\n",
    "        continue\n",
    "\n",
    "    k = res[\"k\"]\n",
    "    coords = res[\"coords\"]  # pca1..pcak\n",
    "    meta = pick_meta(df)\n",
    "    out_df = coords.join(meta, how=\"left\")\n",
    "\n",
    "    # nom fichier sortie\n",
    "    base = str(name)\n",
    "    if base.endswith(\".csv\"):\n",
    "        base = os.path.splitext(os.path.basename(base))[0]\n",
    "    safe_name = base.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    out_path = os.path.join(OUT_DIR_COORDS, f\"{safe_name}_pca_{k}d.csv\")\n",
    "\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"dataset\": safe_name,\n",
    "        \"n_components\": k,\n",
    "        \"explained_variance_cum\": res[\"explained_var\"],\n",
    "        \"trustworthiness_k\": res[\"trustworthiness\"],\n",
    "        \"continuity_k\": res[\"continuity\"],\n",
    "        \"mrre_k\": res[\"mrre\"],\n",
    "        \"out_file\": out_path,\n",
    "    })\n",
    "\n",
    "    print(f\"[OK] {safe_name}: k={k}, var={res['explained_var']:.3f}, \"\n",
    "          f\"trust={res['trustworthiness']:.3f}, dist_corr={res['distance_correlation']:.3f} -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Sauvegarde & aperçu des métriques\n",
    "if metrics_rows:\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    metrics_df.to_csv(OUT_METRICS_CSV, index=False)\n",
    "    display(metrics_df.head())\n",
    "    print(f\"Métriques sauvegardées -> {OUT_METRICS_CSV}\")\n",
    "else:\n",
    "    print(\"Aucun jeu traité.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "datamining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
