{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b12fa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Clustering t-SNE & UMAP — Ligue 1\n",
    "# Cette section charge les données réduites et prépare l'environnement.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# --- Chemins de fichiers (tes chemins) ---\n",
    "file_path_PCA    = \"../../reduced_data/pca/embeddings/joueurs_ligue1_PCA_custom.csv\"\n",
    "file_path_tSNE   = \"../../reduced_data/tsne/embeddings/joueurs_ligue1_tSNE_custom_GK.csv\"\n",
    "file_path_ISOMAP = \"../../reduced_data/isomap/embeddings/joueurs_ligue1_ISOMap_raw.csv\"\n",
    "file_path_UMAP   = \"../../reduced_data/umap/embeddings/joueurs_ligue1_2024_2025_clean_per90_umap3d_best_embedding.csv\"\n",
    "\n",
    "# Dossier sorties\n",
    "OUT_DIR = \"../../cluster_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Colonnes méta à préserver si présentes\n",
    "META_COLS = [\"player_name\", \"positions\", \"equipe\", \"team\", \"tag\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "942ed2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes → PCA=(333, 37), tSNE=(356, 7), ISOMAP=(333, 8), UMAP=(333, 9)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Chargement des jeux de données\n",
    "\n",
    "# %%\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df_pca    = load_dataset(file_path_PCA)\n",
    "df_tsne   = load_dataset(file_path_tSNE)\n",
    "df_isomap = load_dataset(file_path_ISOMAP)\n",
    "df_umap   = load_dataset(file_path_UMAP)\n",
    "\n",
    "print(\"Shapes →\",\n",
    "      f\"PCA={df_pca.shape}, tSNE={df_tsne.shape}, ISOMAP={df_isomap.shape}, UMAP={df_umap.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2072c4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features → PCA=34, tSNE=4, ISOMAP=5, UMAP=4\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Sélection des features (colonnes numériques)\n",
    "\n",
    "# %%\n",
    "def select_feature_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    meta = [c for c in META_COLS if c in df.columns]\n",
    "    X = df.select_dtypes(include=[np.number]).copy()\n",
    "    if X.shape[1] == 0:  # fallback si besoin\n",
    "        X = df.drop(columns=meta, errors=\"ignore\")\n",
    "    return X\n",
    "\n",
    "X_pca    = select_feature_matrix(df_pca)\n",
    "X_tsne   = select_feature_matrix(df_tsne)\n",
    "X_isomap = select_feature_matrix(df_isomap)\n",
    "X_umap   = select_feature_matrix(df_umap)\n",
    "\n",
    "print(\"Num features →\",\n",
    "      f\"PCA={X_pca.shape[1]}, tSNE={X_tsne.shape[1]}, ISOMAP={X_isomap.shape[1]}, UMAP={X_umap.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32dd9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Métriques & export\n",
    "\n",
    "# %%\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def safe_metrics(X: pd.DataFrame, labels: np.ndarray) -> Tuple[Optional[float], Optional[float], Optional[float], int, Optional[int]]:\n",
    "    unique = np.unique(labels)\n",
    "    n_clusters = len(unique[unique != -1]) if -1 in unique else len(unique)\n",
    "    n_noise = int(np.sum(labels == -1)) if -1 in unique else None\n",
    "\n",
    "    if n_clusters <= 1:\n",
    "        return None, None, None, n_clusters, n_noise\n",
    "\n",
    "    try:\n",
    "        sil = silhouette_score(X, labels)\n",
    "    except Exception:\n",
    "        sil = None\n",
    "    try:\n",
    "        ch = calinski_harabasz_score(X, labels)\n",
    "    except Exception:\n",
    "        ch = None\n",
    "    try:\n",
    "        db = davies_bouldin_score(X, labels)\n",
    "    except Exception:\n",
    "        db = None\n",
    "    return sil, ch, db, n_clusters, n_noise\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Export clusters — version complète (métadonnées + coordonnées automatiques)\n",
    "\n",
    "# %%\n",
    "def export_clusters(df_src: pd.DataFrame, labels: np.ndarray, dataset_key: str, method_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Exporte automatiquement :\n",
    "      - les colonnes méta (player_name, positions, équipe, team, tag)\n",
    "      - la colonne 'cluster'\n",
    "      - toutes les colonnes numériques (coordonnées : tsne_*, umap_*, pca_*, etc.)\n",
    "    \"\"\"\n",
    "    meta_cols = [c for c in META_COLS if c in df_src.columns]\n",
    "    num_cols = [c for c in df_src.select_dtypes(include=[np.number]).columns if c != \"cluster\"]\n",
    "\n",
    "    # Construit le DataFrame exporté\n",
    "    out = df_src[meta_cols].copy() if meta_cols else pd.DataFrame(index=df_src.index)\n",
    "    out[\"cluster\"] = labels\n",
    "    for c in num_cols:\n",
    "        out[c] = df_src[c].values\n",
    "\n",
    "    # Sauvegarde automatique\n",
    "    out_path = os.path.join(OUT_DIR, f\"clusters_{dataset_key}_{method_name}.csv\")\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62d2e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Définition des algorithmes (hyperparamètres imposés)\n",
    "\n",
    "# %%\n",
    "def run_affinity_tsne(X: pd.DataFrame) -> np.ndarray:\n",
    "    # Affinity Propagation (t-SNE): damping=0.75, preference=-40, random_state=42\n",
    "    model = AffinityPropagation(damping=0.75, preference=-40, random_state=42)\n",
    "    return model.fit_predict(X)\n",
    "\n",
    "def run_gmm_tsne(X: pd.DataFrame) -> np.ndarray:\n",
    "    # GMM (t-SNE): n_components=5, covariance_type=\"full\", random_state=42\n",
    "    gmm = GaussianMixture(n_components=5, covariance_type=\"full\", random_state=42)\n",
    "    gmm.fit(X)\n",
    "    return gmm.predict(X)\n",
    "\n",
    "def run_dbscan_tsne(X: pd.DataFrame) -> np.ndarray:\n",
    "    # DBSCAN (t-SNE): eps=0.9521711055021824, min_samples=20\n",
    "    db = DBSCAN(eps=0.9521711055021824, min_samples=20)\n",
    "    return db.fit_predict(X)\n",
    "\n",
    "def run_agglomerative_umap(X: pd.DataFrame) -> np.ndarray:\n",
    "    # Agglomerative (UMAP): n_clusters=5, linkage=\"ward\"\n",
    "    agg = AgglomerativeClustering(n_clusters=5, linkage=\"ward\")\n",
    "    return agg.fit_predict(X)\n",
    "\n",
    "def run_kmeans_umap(X: pd.DataFrame) -> np.ndarray:\n",
    "    # KMeans (UMAP): n_clusters=5\n",
    "    km = KMeans(n_clusters=5, n_init=10, random_state=42)\n",
    "    return km.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23663603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Affinity/tSNE] clusters=12, sil=0.305359449009325, ch=232.84069377413527, db=1.1282189966630074 → ../../cluster_results/clusters_tsne_affinity.csv\n",
      "[GMM/tSNE] clusters=5, sil=0.3284210534968433, ch=248.30416988612498, db=1.1642518000146584 → ../../cluster_results/clusters_tsne_gmm.csv\n",
      "[DBSCAN/tSNE] clusters=1, noise=336, sil=None, ch=None, db=None → ../../cluster_results/clusters_tsne_dbscan.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Exécution — t-SNE : Affinity, GMM, DBSCAN\n",
    "\n",
    "# %%\n",
    "results = []\n",
    "\n",
    "# 1) Affinity (t-SNE)\n",
    "labels = run_affinity_tsne(X_tsne)\n",
    "sil, ch, db, n_cl, n_noise = safe_metrics(X_tsne, labels)\n",
    "path = export_clusters(df_tsne, labels, \"tsne\", \"affinity\")\n",
    "results.append({\"method\":\"affinity_tsne\",\"dataset\":\"tsne\",\"n_clusters\":n_cl,\"n_noise\":n_noise,\n",
    "                \"silhouette\":sil,\"calinski_harabasz\":ch,\"davies_bouldin\":db,\"clusters_csv\":path})\n",
    "print(f\"[Affinity/tSNE] clusters={n_cl}, sil={sil}, ch={ch}, db={db} → {path}\")\n",
    "\n",
    "# 2) GMM (t-SNE)\n",
    "labels = run_gmm_tsne(X_tsne)\n",
    "sil, ch, db, n_cl, n_noise = safe_metrics(X_tsne, labels)\n",
    "path = export_clusters(df_tsne, labels, \"tsne\", \"gmm\")\n",
    "results.append({\"method\":\"gmm_tsne\",\"dataset\":\"tsne\",\"n_clusters\":n_cl,\"n_noise\":n_noise,\n",
    "                \"silhouette\":sil,\"calinski_harabasz\":ch,\"davies_bouldin\":db,\"clusters_csv\":path})\n",
    "print(f\"[GMM/tSNE] clusters={n_cl}, sil={sil}, ch={ch}, db={db} → {path}\")\n",
    "\n",
    "# 3) DBSCAN (t-SNE)\n",
    "labels = run_dbscan_tsne(X_tsne)\n",
    "sil, ch, db, n_cl, n_noise = safe_metrics(X_tsne, labels)\n",
    "path = export_clusters(df_tsne, labels, \"tsne\", \"dbscan\")\n",
    "results.append({\"method\":\"dbscan_tsne\",\"dataset\":\"tsne\",\"n_clusters\":n_cl,\"n_noise\":n_noise,\n",
    "                \"silhouette\":sil,\"calinski_harabasz\":ch,\"davies_bouldin\":db,\"clusters_csv\":path})\n",
    "print(f\"[DBSCAN/tSNE] clusters={n_cl}, noise={n_noise}, sil={sil}, ch={ch}, db={db} → {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e24a410c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agglo/UMAP] clusters=5, sil=0.2829845995242783, ch=196.4034339336274, db=1.05369096786117 → ../../cluster_results/clusters_umap_agglomerative.csv\n",
      "[KMeans/UMAP] clusters=5, sil=0.30292913527555065, ch=218.0853831742956, db=1.0237728497593295 → ../../cluster_results/clusters_umap_kmeans_5.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Exécution — UMAP : Agglomerative(ward), KMeans(5)\n",
    "\n",
    "# %%\n",
    "# 4) Agglomerative (UMAP)\n",
    "labels = run_agglomerative_umap(X_umap)\n",
    "sil, ch, db, n_cl, n_noise = safe_metrics(X_umap, labels)\n",
    "path = export_clusters(df_umap, labels, \"umap\", \"agglomerative\")\n",
    "results.append({\"method\":\"agg_umap\",\"dataset\":\"umap\",\"n_clusters\":n_cl,\"n_noise\":n_noise,\n",
    "                \"silhouette\":sil,\"calinski_harabasz\":ch,\"davies_bouldin\":db,\"clusters_csv\":path})\n",
    "print(f\"[Agglo/UMAP] clusters={n_cl}, sil={sil}, ch={ch}, db={db} → {path}\")\n",
    "\n",
    "# 5) KMeans (UMAP)\n",
    "labels = run_kmeans_umap(X_umap)\n",
    "sil, ch, db, n_cl, n_noise = safe_metrics(X_umap, labels)\n",
    "path = export_clusters(df_umap, labels, \"umap\", \"kmeans_5\")\n",
    "results.append({\"method\":\"kmeans_umap\",\"dataset\":\"umap\",\"n_clusters\":n_cl,\"n_noise\":n_noise,\n",
    "                \"silhouette\":sil,\"calinski_harabasz\":ch,\"davies_bouldin\":db,\"clusters_csv\":path})\n",
    "print(f\"[KMeans/UMAP] clusters={n_cl}, sil={sil}, ch={ch}, db={db} → {path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "datamining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
