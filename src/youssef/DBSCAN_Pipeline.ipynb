{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af48d920",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering – Multi-Method Pipeline\n",
    "\n",
    "For each method, we:\n",
    "1) Load the selected embedding (case-insensitive column handling).\n",
    "2) Scale the embedding columns (StandardScaler).\n",
    "3) Build a data-driven eps grid from k-distance percentiles (10%–90%).\n",
    "4) Grid search DBSCAN over (eps, min_samples), compute internal validity metrics:\n",
    "      - Silhouette (↑ better), Calinski–Harabasz (↑ better), Davies–Bouldin (↓ better).\n",
    "5) Select the best config by: Silhouette ↑, then CH ↑, then DB ↓.\n",
    "6) Save grid, best row, cluster assignments, and a plot of the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d3859",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8884a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231d47e",
   "metadata": {},
   "source": [
    "# Constants and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c7e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCED_ROOT = \"reduced_data\"\n",
    "\n",
    "METHODS = [\"umap\", \"pca\", \"tsne\", \"isomap\"]  # case-insensitive handling downstream\n",
    "\n",
    "# Input sub-structure per method\n",
    "UMAP_DIR = os.path.join(REDUCED_ROOT, \"umap\")\n",
    "UMAP_EMB_DIR = os.path.join(UMAP_DIR, \"embeddings\")\n",
    "UMAP_BEST_DIR = os.path.join(UMAP_DIR, \"best_results\")\n",
    "\n",
    "PCA_EMB_DIR = os.path.join(REDUCED_ROOT, \"pca\", \"embeddings\")\n",
    "TSNE_EMB_DIR = os.path.join(REDUCED_ROOT, \"tsne\", \"embeddings\")\n",
    "ISOMAP_EMB_DIR = os.path.join(REDUCED_ROOT, \"isomap\", \"embeddings\")\n",
    "\n",
    "# Output directories\n",
    "CLUST_ROOT = os.path.join(\"clusters\", \"dbscan\")\n",
    "GRID_DIR = os.path.join(CLUST_ROOT, \"grid_search\")\n",
    "BEST_DIR = os.path.join(CLUST_ROOT, \"best_results\")\n",
    "CLUSTERS_DIR = os.path.join(CLUST_ROOT, \"clusters\")\n",
    "PLOTS_DIR = os.path.join(CLUST_ROOT, \"plots\")\n",
    "\n",
    "for d in [CLUST_ROOT, GRID_DIR, BEST_DIR, CLUSTERS_DIR, PLOTS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Grid search hyperparameters\n",
    "MIN_SAMPLES_GRID = [5, 7, 10, 15, 20]\n",
    "K_FOR_KDIST = max(MIN_SAMPLES_GRID)  # use the most stringent min_samples to build eps grid\n",
    "PERCENT_RANGE = (10, 90)             # percentile window for eps candidates\n",
    "N_EPS_CANDIDATES = 15                # number of eps values to scan within percentile window\n",
    "\n",
    "# Known (optional) ID columns to preserve if present\n",
    "KNOWN_ID_COLS = [\n",
    "    \"player_name\", \"equipe\", \"positions\", \"age\",\n",
    "    \"player_id\", \"player_country_code\"\n",
    "]\n",
    "\n",
    "METRIC_PRIORITY = ['silhouette', 'calinski_harabasz', 'davies_bouldin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe86042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07b799",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce3c0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"Save helper with directory creation.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"💾 Saved: {path}\")\n",
    "\n",
    "\n",
    "def to_lowercase_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize all column names to lowercase.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_embedding_columns_old(df: pd.DataFrame, method_prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Given a DataFrame (already lowercased) and a method prefix ('umap_', 'pca_', 'tsne_', 'isomap_'),\n",
    "    return the list of embedding columns of the form f'{prefix}<int>' (e.g., 'umap_1', 'umap_2', ...).\n",
    "    \"\"\"\n",
    "    pat = re.compile(rf\"^{re.escape(method_prefix)}(\\d+)$\")\n",
    "    emb_cols = [c for c in df.columns if pat.match(c)]\n",
    "    # numerical suffix order\n",
    "    emb_cols = sorted(emb_cols, key=lambda x: int(pat.match(x).group(1)))\n",
    "    return emb_cols\n",
    "\n",
    "# new\n",
    "def detect_embedding_columns(df: pd.DataFrame, method_prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Detect embedding columns for a method, accepting both underscore and no-underscore styles:\n",
    "      - 'pca_1', 'pca_2', ...  or  'pca1', 'pca2', ...\n",
    "    Case-insensitive; df is assumed lowercased.\n",
    "    \"\"\"\n",
    "    # optional underscore after prefix, then integer suffix\n",
    "    # e.g., ^pca_?(\\d+)$, ^umap_?(\\d+)$, ^tsne_?(\\d+)$, ^isomap_?(\\d+)$\n",
    "    pat = re.compile(rf\"^{re.escape(method_prefix)}_?(\\d+)$\")\n",
    "    emb_cols = []\n",
    "    for c in df.columns:\n",
    "        m = pat.match(c)\n",
    "        if m:\n",
    "            emb_cols.append((c, int(m.group(1))))\n",
    "    emb_cols = [name for name, _ in sorted(emb_cols, key=lambda t: t[1])]\n",
    "    return emb_cols\n",
    "\n",
    "\n",
    "def pick_id_columns(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Select ID columns if present; fall back to all non-numeric columns (excluding embedding columns).\n",
    "    \"\"\"\n",
    "    ids = [c for c in KNOWN_ID_COLS if c in df.columns]\n",
    "    if ids:\n",
    "        return ids\n",
    "    # Fallback: non-numeric cols\n",
    "    non_num = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    return non_num\n",
    "\n",
    "\n",
    "def scale_embedding(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standardize embedding features before clustering.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "def k_distance_values(X_scaled: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sorted k-distances for eps grid building.\n",
    "    Returns the sorted distances to the k-th nearest neighbor for each point.\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, n_jobs=None)\n",
    "    nbrs.fit(X_scaled)\n",
    "    distances, _ = nbrs.kneighbors(X_scaled)\n",
    "    kdist = np.sort(distances[:, k - 1])\n",
    "    return kdist\n",
    "\n",
    "\n",
    "def build_eps_grid_from_percentiles(kdist: np.ndarray,\n",
    "                                    prange=(10, 90),\n",
    "                                    n_candidates=15) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build eps candidate values from percentile window of k-distances.\n",
    "    Handles degenerate ranges.\n",
    "    \"\"\"\n",
    "    p_low, p_high = np.percentile(kdist, prange)\n",
    "    if not np.isfinite(p_low) or not np.isfinite(p_high):\n",
    "        # Fallback to median ± small deltas\n",
    "        med = np.median(kdist)\n",
    "        return np.linspace(max(1e-6, med * 0.5), med * 1.5, n_candidates)\n",
    "\n",
    "    if p_high <= p_low:\n",
    "        # Flat curve; expand slightly around p_low\n",
    "        base = p_low if np.isfinite(p_low) else np.median(kdist)\n",
    "        eps_min = max(1e-6, base * 0.8)\n",
    "        eps_max = base * 1.2\n",
    "        return np.linspace(eps_min, eps_max, n_candidates)\n",
    "\n",
    "    return np.linspace(max(1e-6, p_low), p_high, n_candidates)\n",
    "\n",
    "\n",
    "def evaluate_labels(X_scaled: np.ndarray, labels: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute internal validation metrics on non-noise points if ≥2 clusters.\n",
    "    Returns dict with (silhouette, ch, db) possibly NaN if not meaningful.\n",
    "    \"\"\"\n",
    "    mask = labels != -1\n",
    "    # cluster count excluding noise\n",
    "    clusters = set(labels[mask])\n",
    "    n_clusters = len(clusters)\n",
    "    n_noise = int((labels == -1).sum())\n",
    "\n",
    "    metrics = {\"silhouette\": np.nan, \"calinski_harabasz\": np.nan, \"davies_bouldin\": np.nan,\n",
    "               \"n_clusters\": int(n_clusters), \"n_noise\": n_noise}\n",
    "\n",
    "    if mask.sum() > 1 and n_clusters > 1:\n",
    "        Xv = X_scaled[mask]\n",
    "        yv = labels[mask]\n",
    "        try:\n",
    "            sil = silhouette_score(Xv, yv)\n",
    "            ch = calinski_harabasz_score(Xv, yv)\n",
    "            db = davies_bouldin_score(Xv, yv)\n",
    "            metrics.update({\n",
    "                \"silhouette\": float(sil),\n",
    "                \"calinski_harabasz\": float(ch),\n",
    "                \"davies_bouldin\": float(db)\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def lexicographic_best(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Select best row by:\n",
    "      1) silhouette: max\n",
    "      2) calinski_harabasz: max\n",
    "      3) davies_bouldin: min\n",
    "    Rows with NaN silhouette are ranked last automatically.\n",
    "    \"\"\"\n",
    "    ranked = (\n",
    "        df.assign(\n",
    "            silhouette_rank=df[\"silhouette\"].rank(method=\"min\", ascending=False, na_option=\"bottom\"),\n",
    "            ch_rank=df[\"calinski_harabasz\"].rank(method=\"min\", ascending=False, na_option=\"bottom\"),\n",
    "            db_rank=df[\"davies_bouldin\"].rank(method=\"min\", ascending=True, na_option=\"bottom\")\n",
    "        )\n",
    "        .sort_values(by=[\"silhouette_rank\", \"ch_rank\", \"db_rank\"], ascending=True)\n",
    "    )\n",
    "    return ranked.iloc[0]\n",
    "\n",
    "\n",
    "def plot_best_scatter(df_full: pd.DataFrame,\n",
    "                      emb_cols: list,\n",
    "                      labels_col: str,\n",
    "                      title: str,\n",
    "                      outpath: str) -> None:\n",
    "    \"\"\"Scatter plot of first two embedding dims colored by cluster label.\"\"\"\n",
    "    if len(emb_cols) < 2:\n",
    "        print(\"⚠️ Less than 2 embedding dimensions – skipping plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n_clusters = len(set(df_full[labels_col])) - (1 if -1 in df_full[labels_col].values else 0)\n",
    "    palette = sns.color_palette(None, max(1, n_clusters))\n",
    "    sns.scatterplot(\n",
    "        data=df_full,\n",
    "        x=emb_cols[0], y=emb_cols[1],\n",
    "        hue=labels_col, palette=palette, s=45, alpha=0.9, edgecolor=\"none\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"📈 Saved plot: {outpath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de54142",
   "metadata": {},
   "source": [
    "# UMAP Best-Embedding Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58820d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_umap_embedding_path() -> str:\n",
    "    \"\"\"\n",
    "    Among files in `umap/best_results/`, pick the CSV (per dataset) with the **lowest MSE**,\n",
    "    then find the corresponding embedding CSV under `umap/embeddings/`.\n",
    "\n",
    "    Assumptions from `umap_pipeline.py`:\n",
    "    - Best metrics file name: `{tag}_umap_metrics.csv`\n",
    "    - Embedding file saved as: `embeddings/{tag}_umap{Nd}d_best_embedding.csv`\n",
    "      (we will search for files starting with `{tag}_` and containing `best_embedding`)\n",
    "\n",
    "    Returns: absolute path to the chosen embedding CSV, or raises if not found.\n",
    "    \"\"\"\n",
    "    best_files = glob.glob(os.path.join(UMAP_BEST_DIR, \"*.csv\"))\n",
    "    if not best_files:\n",
    "        raise FileNotFoundError(f\"No best_results CSVs found in {UMAP_BEST_DIR}\")\n",
    "\n",
    "    # Read all metrics and pick the single *global* best (lowest MSE) across datasets\n",
    "    rows = []\n",
    "    for bf in best_files:\n",
    "        try:\n",
    "            dfm = pd.read_csv(bf)\n",
    "            if \"mse\" not in dfm.columns or dfm.empty:\n",
    "                continue\n",
    "            mse = float(dfm[\"mse\"].iloc[0])\n",
    "            rows.append({\"path\": bf, \"mse\": mse})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid UMAP best_results rows with MSE found.\")\n",
    "\n",
    "    best_entry = min(rows, key=lambda r: r[\"mse\"])\n",
    "    best_path = best_entry[\"path\"]\n",
    "    tag = os.path.basename(best_path).replace(\"_umap_metrics.csv\", \"\")\n",
    "\n",
    "    # Find the embedding file matching this tag\n",
    "    candidates = glob.glob(os.path.join(UMAP_EMB_DIR, f\"{tag}_*best_embedding*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No embedding CSV with best_embedding found for tag '{tag}' in {UMAP_EMB_DIR}\")\n",
    "\n",
    "    # If multiple, prefer the one that mentions dimensionality implied by metrics row (if present)\n",
    "    # Otherwise, take the first sorted candidate for determinism\n",
    "    candidates.sort()\n",
    "    chosen = candidates[0]\n",
    "    print(f\"✅ Selected UMAP embedding for DBSCAN: {os.path.basename(chosen)} (from tag: {tag})\")\n",
    "    return chosen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff4de6",
   "metadata": {},
   "source": [
    "# Metric-based Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbc432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_by_metrics(df: pd.DataFrame, priority_list: list[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Select best DBSCAN configuration dynamically based on user-specified priority of metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Grid search results containing eps, min_samples, and metrics.\n",
    "    priority_list : list[str]\n",
    "        Metrics to prioritize in order, e.g. ['silhouette', 'calinski_harabasz', 'davies_bouldin'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The best row according to the chosen priority.\n",
    "    \"\"\"\n",
    "    ranked_df = df.copy()\n",
    "\n",
    "    # For each metric, compute a ranking column dynamically\n",
    "    for metric in priority_list:\n",
    "        if metric not in df.columns:\n",
    "            print(f\"⚠️ Metric '{metric}' not found in grid; skipping.\")\n",
    "            continue\n",
    "\n",
    "        ascending = metric.lower() == 'davies_bouldin'  # DB index → lower is better\n",
    "        rank_col = f\"{metric}_rank\"\n",
    "        ranked_df[rank_col] = ranked_df[metric].rank(\n",
    "            method=\"min\", ascending=ascending, na_option=\"bottom\"\n",
    "        )\n",
    "\n",
    "    # Sort by all rank columns in order of priority\n",
    "    rank_cols = [f\"{m}_rank\" for m in priority_list if f\"{m}_rank\" in ranked_df.columns]\n",
    "    if not rank_cols:\n",
    "        raise ValueError(\"No valid metrics found for ranking in the grid DataFrame.\")\n",
    "\n",
    "    ranked_df = ranked_df.sort_values(by=rank_cols, ascending=True)\n",
    "    best_row = ranked_df.iloc[0]\n",
    "    return best_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02c747",
   "metadata": {},
   "source": [
    "# Method → Embedding File Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae78110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_one_csv_from_dir_by_method(emb_dir: str, method_keyword: str) -> str:\n",
    "    \"\"\"\n",
    "    Pick one CSV from a directory that contains the method keyword (case-insensitive).\n",
    "    Preference order if multiple:\n",
    "      1) filenames containing 'custom'\n",
    "      2) filenames containing 'raw'\n",
    "      3) lexicographically first\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        raise FileNotFoundError(f\"Embeddings directory not found: {emb_dir}\")\n",
    "\n",
    "    all_csv = glob.glob(os.path.join(emb_dir, \"*.csv\"))\n",
    "    cand = [p for p in all_csv if method_keyword.lower() in os.path.basename(p).lower()]\n",
    "    if not cand:\n",
    "        # fallback: if method_keyword not in names (e.g., teammates exported without it), use any csv\n",
    "        cand = all_csv.copy()\n",
    "\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No CSV embeddings found in {emb_dir}\")\n",
    "\n",
    "    # preferences\n",
    "    prefer_custom = [p for p in cand if \"custom\" in os.path.basename(p).lower()]\n",
    "    if prefer_custom:\n",
    "        prefer_custom.sort()\n",
    "        return prefer_custom[0]\n",
    "\n",
    "    prefer_raw = [p for p in cand if \"raw\" in os.path.basename(p).lower()]\n",
    "    if prefer_raw:\n",
    "        prefer_raw.sort()\n",
    "        return prefer_raw[0]\n",
    "\n",
    "    cand.sort()\n",
    "    return cand[0]\n",
    "\n",
    "\n",
    "# def resolve_embedding_file_for_method(method: str) -> str:\n",
    "#     m = method.lower()\n",
    "#     if m == \"umap\":\n",
    "#         return select_best_umap_embedding_path()\n",
    "#     elif m == \"pca\":\n",
    "#         return _pick_one_csv_from_dir_by_method(PCA_EMB_DIR, \"pca\")\n",
    "#     elif m == \"tsne\":\n",
    "#         # cover tsne/tSNE naming\n",
    "#         return _pick_one_csv_from_dir_by_method(TSNE_EMB_DIR, \"tsne\")\n",
    "#     elif m == \"isomap\":\n",
    "#         return _pick_one_csv_from_dir_by_method(ISOMAP_EMB_DIR, \"isomap\")\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "def resolve_embedding_file_for_method(method: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolve the correct embedding file based on the intended dataset source per method:\n",
    "      - UMAP  → lowest-MSE embedding (per90 dataset)\n",
    "      - PCA   → embeddings containing 'custom'\n",
    "      - t-SNE → embeddings containing 'custom_gk'\n",
    "      - ISOMAP→ embeddings containing 'raw'\n",
    "    \"\"\"\n",
    "    m = method.lower()\n",
    "\n",
    "    if m == \"umap\":\n",
    "        return select_best_umap_embedding_path()\n",
    "\n",
    "    if m == \"pca\":\n",
    "        pattern = \"*custom*.csv\"\n",
    "        base_dir = PCA_EMB_DIR\n",
    "    elif m == \"tsne\":\n",
    "        pattern = \"*custom_gk*.csv\"\n",
    "        base_dir = TSNE_EMB_DIR\n",
    "    elif m == \"isomap\":\n",
    "        pattern = \"*raw*.csv\"\n",
    "        base_dir = ISOMAP_EMB_DIR\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    candidates = sorted(glob.glob(os.path.join(base_dir, pattern)))\n",
    "    if not candidates:\n",
    "        # fallback: pick any embedding for the method if specific keyword missing\n",
    "        print(f\"⚠️ No embeddings matching {pattern} found for {m.upper()}, falling back to first available.\")\n",
    "        candidates = sorted(glob.glob(os.path.join(base_dir, \"*.csv\")))\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError(f\"No embeddings found in {base_dir} for {m.upper()}\")\n",
    "\n",
    "    chosen = candidates[0]\n",
    "    print(f\"✅ Selected {m.upper()} embedding: {os.path.basename(chosen)}\")\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def resolve_all_embedding_files_for_method(method: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of all embedding CSV paths for a given method.\n",
    "\n",
    "    Each method folder can contain multiple embeddings (e.g. multiple UMAPs).\n",
    "    We gather *all* CSVs under its embeddings/ subdirectory.\n",
    "\n",
    "    Returns a list of absolute file paths.\n",
    "    \"\"\"\n",
    "    m = method.lower()\n",
    "    base_dir = None\n",
    "    if m == \"umap\":\n",
    "        base_dir = UMAP_EMB_DIR\n",
    "    elif m == \"pca\":\n",
    "        base_dir = PCA_EMB_DIR\n",
    "    elif m == \"tsne\":\n",
    "        base_dir = TSNE_EMB_DIR\n",
    "    elif m == \"isomap\":\n",
    "        base_dir = ISOMAP_EMB_DIR\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    if not os.path.isdir(base_dir):\n",
    "        raise FileNotFoundError(f\"Embeddings directory not found: {base_dir}\")\n",
    "\n",
    "    all_csvs = sorted(glob.glob(os.path.join(base_dir, \"*.csv\")))\n",
    "    if not all_csvs:\n",
    "        raise FileNotFoundError(f\"No embedding CSVs found for {method.upper()} in {base_dir}\")\n",
    "\n",
    "    print(f\"→ Found {len(all_csvs)} embeddings for {method.upper()} in {base_dir}\")\n",
    "    return all_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b5be6",
   "metadata": {},
   "source": [
    "# Loading & Preparing an Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c20b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_embedding(embedding_csv_path: str, method: str) -> tuple[pd.DataFrame, list, list]:\n",
    "    \"\"\"\n",
    "    Load the embedding CSV, lowercase columns, detect embedding columns by prefix,\n",
    "    and return (df_lower, emb_cols, id_cols).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(embedding_csv_path)\n",
    "    df = to_lowercase_columns(df)\n",
    "\n",
    "    prefix = method.lower()\n",
    "    # standardize known variants (just to be defensive)\n",
    "    if prefix in [\"t-sne\", \"tsne\"]:\n",
    "        prefix = \"tsne\"\n",
    "    if prefix not in [\"umap\", \"pca\", \"tsne\", \"isomap\"]:\n",
    "        raise ValueError(f\"Unsupported method prefix: {prefix}\")\n",
    "\n",
    "    # emb_cols = detect_embedding_columns(df, prefix + \"_\")\n",
    "    emb_cols = detect_embedding_columns(df, prefix)\n",
    "    \n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(f\"No embedding columns like '{prefix}1'/ detected in {embedding_csv_path}\")\n",
    "\n",
    "    id_cols = pick_id_columns(df)\n",
    "    return df, emb_cols, id_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0854599",
   "metadata": {},
   "source": [
    "# DBSCAN Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27477362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_dbscan_on_embedding(df: pd.DataFrame,\n",
    "                                    emb_cols: list,\n",
    "                                    method: str,\n",
    "                                    tag: str) -> tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run grid search over DBSCAN on the selected embedding columns.\n",
    "    Returns:\n",
    "      - grid_df: full table of (eps, min_samples, n_clusters, n_noise, silhouette, ch, db)\n",
    "      - best_row: selected best configuration (Series)\n",
    "      - df_best: original df with a new 'cluster' column for the best config\n",
    "    \"\"\"\n",
    "    X = df[emb_cols].to_numpy(dtype=float, copy=True)\n",
    "    X_scaled = scale_embedding(X)\n",
    "\n",
    "    # Build eps candidates from k-distance\n",
    "    try:\n",
    "        kdist = k_distance_values(X_scaled, k=K_FOR_KDIST)\n",
    "        eps_candidates = build_eps_grid_from_percentiles(\n",
    "            kdist, prange=PERCENT_RANGE, n_candidates=N_EPS_CANDIDATES\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not build eps grid from k-distance ({e}); using fallback linspace.\")\n",
    "        # Fallback: simple range based on std of pairwise distances\n",
    "        eps_candidates = np.linspace(0.1, 2.0, N_EPS_CANDIDATES)\n",
    "\n",
    "    rows = []\n",
    "    for ms in MIN_SAMPLES_GRID:\n",
    "        for eps in eps_candidates:\n",
    "            model = DBSCAN(eps=float(eps), min_samples=int(ms))\n",
    "            labels = model.fit_predict(X_scaled)\n",
    "            metrics = evaluate_labels(X_scaled, labels)\n",
    "            rows.append({\n",
    "                \"method\": method,\n",
    "                \"tag\": tag,\n",
    "                \"eps\": float(eps),\n",
    "                \"min_samples\": int(ms),\n",
    "                **metrics\n",
    "            })\n",
    "\n",
    "    grid_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Select best lexicographically\n",
    "    # best_row = lexicographic_best(grid_df)\n",
    "    best_row = select_best_by_metrics(grid_df, METRIC_PRIORITY)\n",
    "\n",
    "    # Refit best to collect labels for saving\n",
    "    best_eps = float(best_row[\"eps\"])\n",
    "    best_ms = int(best_row[\"min_samples\"])\n",
    "    best_model = DBSCAN(eps=best_eps, min_samples=best_ms)\n",
    "    best_labels = best_model.fit_predict(X_scaled)\n",
    "\n",
    "    df_best = df.copy()\n",
    "    df_best[\"cluster\"] = best_labels\n",
    "\n",
    "    return grid_df, best_row, df_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50533313",
   "metadata": {},
   "source": [
    "# Orchestration per Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdeb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_method(method: str) -> None:\n",
    "    \"\"\"\n",
    "    Resolve the single embedding to use, run DBSCAN grid search, select best model,\n",
    "    and save grid, best, clusters, and plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing method: {method.upper()} ===\")\n",
    "    emb_path = resolve_embedding_file_for_method(method)\n",
    "    tag = os.path.splitext(os.path.basename(emb_path))[0]\n",
    "\n",
    "    print(f\"→ Embedding file: {emb_path}\")\n",
    "\n",
    "    df, emb_cols, id_cols = load_and_prepare_embedding(emb_path, method)\n",
    "    print(\n",
    "        f\"Detected embedding columns ({len(emb_cols)}): {emb_cols[:8]}{' ...' if len(emb_cols) > 8 else ''}\")\n",
    "    print(f\"ID columns retained: {id_cols}\")\n",
    "\n",
    "    # --- Run DBSCAN grid search ---\n",
    "    grid_df, best_row, df_best = grid_search_dbscan_on_embedding(df, emb_cols, method, tag)\n",
    "\n",
    "    # --- Define method-specific output paths ---\n",
    "    grid_out = os.path.join(GRID_DIR, method, f\"{tag}_dbscan_grid.csv\")\n",
    "    best_out = os.path.join(BEST_DIR, method, f\"{tag}_dbscan_best.csv\")\n",
    "    clusters_out = os.path.join(CLUSTERS_DIR, method, f\"{tag}_dbscan_clusters.csv\")\n",
    "    plot_out = os.path.join(PLOTS_DIR, method, f\"{tag}_dbscan_best.png\")\n",
    "\n",
    "    # --- Ensure directories exist BEFORE saving ---\n",
    "    for p in [grid_out, best_out, clusters_out, plot_out]:\n",
    "        os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "\n",
    "    # --- Save results ---\n",
    "    keep_cols = id_cols + emb_cols + [\"cluster\"]\n",
    "    keep_cols = [c for c in keep_cols if c in df_best.columns]\n",
    "    save_csv(grid_df, grid_out)\n",
    "    save_csv(pd.DataFrame([best_row]), best_out)\n",
    "    save_csv(df_best[keep_cols], clusters_out)\n",
    "\n",
    "    # --- Plot scatter ---\n",
    "    title = (f\"{method.upper()} – DBSCAN Best \"\n",
    "             f\"(eps={best_row['eps']:.3f}, min_samples={int(best_row['min_samples'])})\")\n",
    "    plot_best_scatter(df_best, emb_cols, \"cluster\", title, plot_out)\n",
    "\n",
    "    # Visualize which player positions dominate each cluster (excluding noise).\n",
    "    try:\n",
    "        if \"positions\" in df_best.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.countplot(\n",
    "                data=df_best[df_best[\"cluster\"] != -1],\n",
    "                x=\"cluster\", hue=\"positions\", palette=\"tab10\"\n",
    "            )\n",
    "            plt.title(f\"{method.upper()} – Positions per Cluster (excluding noise)\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "            plt.tight_layout()\n",
    "            comp_plot = os.path.join(PLOTS_DIR, method, f\"{tag}_positions_per_cluster.png\")\n",
    "            os.makedirs(os.path.dirname(comp_plot), exist_ok=True)\n",
    "            plt.savefig(comp_plot, dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"📊 Saved composition plot: {comp_plot}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not plot cluster composition: {e}\")\n",
    "\n",
    "    # --- Console summary ---\n",
    "    print(\"\\n🏆 Best configuration:\")\n",
    "    print(f\"  eps={best_row['eps']:.4f} | min_samples={int(best_row['min_samples'])}\")\n",
    "    print(f\"  silhouette={best_row['silhouette']:.3f} | \"\n",
    "          f\"CH={best_row['calinski_harabasz']:.1f} | \"\n",
    "          f\"DB={best_row['davies_bouldin']:.3f}\")\n",
    "    print(f\"  n_clusters={int(best_row['n_clusters'])} | n_noise={int(best_row['n_noise'])}\")\n",
    "    print(f\"✅ Outputs:\\n\"\n",
    "          f\"    grid → {grid_out}\\n\"\n",
    "          f\"    best → {best_out}\\n\"\n",
    "          f\"    clusters → {clusters_out}\\n\"\n",
    "          f\"    plots → {plot_out}\")\n",
    "    print(\"    extra → composition & silhouette stability plots added ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d1189",
   "metadata": {},
   "source": [
    "# Run on All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a809b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_embeddings_per_method(method: str) -> None:\n",
    "    all_embeddings = resolve_all_embedding_files_for_method(method)\n",
    "    for emb_path in all_embeddings:\n",
    "        try:\n",
    "            tag = os.path.splitext(os.path.basename(emb_path))[0]\n",
    "            print(f\"\\n=== Processing {method.upper()} embedding: {tag} ===\")\n",
    "\n",
    "            df, emb_cols, id_cols = load_and_prepare_embedding(emb_path, method)\n",
    "            print(\n",
    "                f\"Detected embedding columns ({len(emb_cols)}): {emb_cols[:8]}{' ...' if len(emb_cols) > 8 else ''}\")\n",
    "            print(f\"ID columns retained: {id_cols}\")\n",
    "\n",
    "            # Run DBSCAN grid search\n",
    "            grid_df, best_row, df_best = grid_search_dbscan_on_embedding(\n",
    "                df, emb_cols, method, tag\n",
    "            )\n",
    "\n",
    "            # --- Define paths ---\n",
    "            grid_out = os.path.join(GRID_DIR, method, f\"{tag}_dbscan_grid.csv\")\n",
    "            best_out = os.path.join(BEST_DIR, method, f\"{tag}_dbscan_best.csv\")\n",
    "            clusters_out = os.path.join(CLUSTERS_DIR, method, f\"{tag}_dbscan_clusters.csv\")\n",
    "            plot_out = os.path.join(PLOTS_DIR, method, f\"{tag}_dbscan_best.png\")\n",
    "            comp_plot = os.path.join(PLOTS_DIR, method, f\"{tag}_positions_per_cluster.png\")\n",
    "\n",
    "            # --- Ensure directories exist BEFORE saving ---\n",
    "            for p in [grid_out, best_out, clusters_out, plot_out, comp_plot]:\n",
    "                os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "\n",
    "            # --- Save results ---\n",
    "            keep_cols = id_cols + emb_cols + [\"cluster\"]\n",
    "            keep_cols = [c for c in keep_cols if c in df_best.columns]\n",
    "            save_csv(grid_df, grid_out)\n",
    "            save_csv(pd.DataFrame([best_row]), best_out)\n",
    "            save_csv(df_best[keep_cols], clusters_out)\n",
    "\n",
    "            # --- Plot best scatter ---\n",
    "            title = (f\"{method.upper()} – DBSCAN Best \"\n",
    "                     f\"(eps={best_row['eps']:.3f}, min_samples={int(best_row['min_samples'])})\")\n",
    "            plot_best_scatter(df_best, emb_cols, \"cluster\", title, plot_out)\n",
    "\n",
    "            # Optional composition plot\n",
    "            try:\n",
    "                if \"positions\" in df_best.columns:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.countplot(\n",
    "                        data=df_best[df_best[\"cluster\"] != -1],\n",
    "                        x=\"cluster\", hue=\"positions\", palette=\"tab10\"\n",
    "                    )\n",
    "                    plt.title(f\"{method.upper()} – Positions per Cluster (excluding noise)\")\n",
    "                    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "                    os.makedirs(os.path.dirname(comp_plot), exist_ok=True)\n",
    "                    plt.savefig(comp_plot, dpi=300)\n",
    "                    plt.close()\n",
    "                    print(f\"📊 Saved composition plot: {comp_plot}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not plot cluster composition for {tag}: {e}\")\n",
    "\n",
    "            # Console summary\n",
    "            print(\"\\n🏆 Best configuration:\")\n",
    "            print(\n",
    "                f\"  eps={best_row['eps']:.4f} | min_samples={int(best_row['min_samples'])}\")\n",
    "            print(f\"  silhouette={best_row['silhouette']:.3f} | \"\n",
    "                    f\"CH={best_row['calinski_harabasz']:.1f} | \"\n",
    "                    f\"DB={best_row['davies_bouldin']:.3f}\")\n",
    "            print(\n",
    "                f\"  n_clusters={int(best_row['n_clusters'])} | n_noise={int(best_row['n_noise'])}\")\n",
    "            print(f\"✅ Outputs:\\n\"\n",
    "                    f\"    grid → {grid_out}\\n\"\n",
    "                    f\"    best → {best_out}\\n\"\n",
    "                    f\"    clusters → {clusters_out}\\n\"\n",
    "                    f\"    plots → {plot_out}\")\n",
    "            print(\"    extra → composition plots added ✅\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping embedding {emb_path} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbec002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning reduced embeddings under: reduced_data\n",
      "\n",
      "=== Processing method: UMAP ===\n",
      "✅ Selected UMAP embedding for DBSCAN: joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding.csv (from tag: joueurs_ligue1_2024_2025_clean_per90)\n",
      "→ Embedding file: reduced_data\\umap\\embeddings\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding.csv\n",
      "Detected embedding columns (5): ['umap_1', 'umap_2', 'umap_3', 'umap_4', 'umap_5']\n",
      "ID columns retained: ['player_name', 'equipe', 'positions', 'age', 'player_id', 'player_country_code']\n",
      "💾 Saved: clusters\\dbscan\\grid_search\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_grid.csv\n",
      "💾 Saved: clusters\\dbscan\\best_results\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_best.csv\n",
      "💾 Saved: clusters\\dbscan\\clusters\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_clusters.csv\n",
      "📈 Saved plot: clusters\\dbscan\\plots\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_best.png\n",
      "📊 Saved composition plot: clusters\\dbscan\\plots\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_positions_per_cluster.png\n",
      "\n",
      "🏆 Best configuration:\n",
      "  eps=1.0640 | min_samples=20\n",
      "  silhouette=0.404 | CH=200.7 | DB=0.880\n",
      "  n_clusters=2 | n_noise=29\n",
      "✅ Outputs:\n",
      "    grid → clusters\\dbscan\\grid_search\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_grid.csv\n",
      "    best → clusters\\dbscan\\best_results\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_best.csv\n",
      "    clusters → clusters\\dbscan\\clusters\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_clusters.csv\n",
      "    plots → clusters\\dbscan\\plots\\umap\\joueurs_ligue1_2024_2025_clean_per90_umap5d_best_embedding_dbscan_best.png\n",
      "    extra → composition & silhouette stability plots added ✅\n",
      "\n",
      "=== Processing method: PCA ===\n",
      "✅ Selected PCA embedding: joueurs_ligue1_PCA_custom.csv\n",
      "→ Embedding file: reduced_data\\pca\\embeddings\\joueurs_ligue1_PCA_custom.csv\n",
      "Detected embedding columns (34): ['pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8'] ...\n",
      "ID columns retained: ['player_name', 'equipe', 'positions']\n",
      "💾 Saved: clusters\\dbscan\\grid_search\\pca\\joueurs_ligue1_PCA_custom_dbscan_grid.csv\n",
      "💾 Saved: clusters\\dbscan\\best_results\\pca\\joueurs_ligue1_PCA_custom_dbscan_best.csv\n",
      "💾 Saved: clusters\\dbscan\\clusters\\pca\\joueurs_ligue1_PCA_custom_dbscan_clusters.csv\n",
      "📈 Saved plot: clusters\\dbscan\\plots\\pca\\joueurs_ligue1_PCA_custom_dbscan_best.png\n",
      "📊 Saved composition plot: clusters\\dbscan\\plots\\pca\\joueurs_ligue1_PCA_custom_positions_per_cluster.png\n",
      "\n",
      "🏆 Best configuration:\n",
      "  eps=5.0381 | min_samples=5\n",
      "  silhouette=nan | CH=nan | DB=nan\n",
      "  n_clusters=1 | n_noise=159\n",
      "✅ Outputs:\n",
      "    grid → clusters\\dbscan\\grid_search\\pca\\joueurs_ligue1_PCA_custom_dbscan_grid.csv\n",
      "    best → clusters\\dbscan\\best_results\\pca\\joueurs_ligue1_PCA_custom_dbscan_best.csv\n",
      "    clusters → clusters\\dbscan\\clusters\\pca\\joueurs_ligue1_PCA_custom_dbscan_clusters.csv\n",
      "    plots → clusters\\dbscan\\plots\\pca\\joueurs_ligue1_PCA_custom_dbscan_best.png\n",
      "    extra → composition & silhouette stability plots added ✅\n",
      "\n",
      "=== Processing method: TSNE ===\n",
      "✅ Selected TSNE embedding: joueurs_ligue1_tSNE_custom_GK.csv\n",
      "→ Embedding file: reduced_data\\tsne\\embeddings\\joueurs_ligue1_tSNE_custom_GK.csv\n",
      "Detected embedding columns (4): ['tsne_1', 'tsne_2', 'tsne_3', 'tsne_4']\n",
      "ID columns retained: ['player_name', 'equipe', 'positions']\n",
      "💾 Saved: clusters\\dbscan\\grid_search\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_grid.csv\n",
      "💾 Saved: clusters\\dbscan\\best_results\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_best.csv\n",
      "💾 Saved: clusters\\dbscan\\clusters\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_clusters.csv\n",
      "📈 Saved plot: clusters\\dbscan\\plots\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_best.png\n",
      "📊 Saved composition plot: clusters\\dbscan\\plots\\tsne\\joueurs_ligue1_tSNE_custom_GK_positions_per_cluster.png\n",
      "\n",
      "🏆 Best configuration:\n",
      "  eps=0.7281 | min_samples=20\n",
      "  silhouette=0.607 | CH=420.9 | DB=0.572\n",
      "  n_clusters=4 | n_noise=215\n",
      "✅ Outputs:\n",
      "    grid → clusters\\dbscan\\grid_search\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_grid.csv\n",
      "    best → clusters\\dbscan\\best_results\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_best.csv\n",
      "    clusters → clusters\\dbscan\\clusters\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_clusters.csv\n",
      "    plots → clusters\\dbscan\\plots\\tsne\\joueurs_ligue1_tSNE_custom_GK_dbscan_best.png\n",
      "    extra → composition & silhouette stability plots added ✅\n",
      "\n",
      "=== Processing method: ISOMAP ===\n",
      "✅ Selected ISOMAP embedding: joueurs_ligue1_ISOMap_raw.csv\n",
      "→ Embedding file: reduced_data\\isomap\\embeddings\\joueurs_ligue1_ISOMap_raw.csv\n",
      "Detected embedding columns (5): ['isomap_1', 'isomap_2', 'isomap_3', 'isomap_4', 'isomap_5']\n",
      "ID columns retained: ['player_name', 'equipe', 'positions']\n",
      "💾 Saved: clusters\\dbscan\\grid_search\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_grid.csv\n",
      "💾 Saved: clusters\\dbscan\\best_results\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_best.csv\n",
      "💾 Saved: clusters\\dbscan\\clusters\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_clusters.csv\n",
      "📈 Saved plot: clusters\\dbscan\\plots\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_best.png\n",
      "📊 Saved composition plot: clusters\\dbscan\\plots\\isomap\\joueurs_ligue1_ISOMap_raw_positions_per_cluster.png\n",
      "\n",
      "🏆 Best configuration:\n",
      "  eps=0.7234 | min_samples=7\n",
      "  silhouette=0.407 | CH=51.7 | DB=0.751\n",
      "  n_clusters=2 | n_noise=176\n",
      "✅ Outputs:\n",
      "    grid → clusters\\dbscan\\grid_search\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_grid.csv\n",
      "    best → clusters\\dbscan\\best_results\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_best.csv\n",
      "    clusters → clusters\\dbscan\\clusters\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_clusters.csv\n",
      "    plots → clusters\\dbscan\\plots\\isomap\\joueurs_ligue1_ISOMap_raw_dbscan_best.png\n",
      "    extra → composition & silhouette stability plots added ✅\n"
     ]
    }
   ],
   "source": [
    "print(f\"Scanning reduced embeddings under: {REDUCED_ROOT}\")\n",
    "for method in METHODS:\n",
    "    try:\n",
    "        process_method(method)\n",
    "        # process_all_embeddings_per_method(method)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Skipping {method.upper()} due to error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
